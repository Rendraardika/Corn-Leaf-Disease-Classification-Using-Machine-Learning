# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fUq9vSZDGorYymPLAlXccz7dl5uXNBb7

# Corn Leaf Disease Detection with Enhanced KNN (EKNN)

Pipeline:

1. Pre-processing (normalisasi, grayscale, reduksi noise)
2. Segmentasi dengan **Otsu Thresholding**
3. Ekstraksi fitur (Fine, Coarse, DOR)
4. Klasifikasi dengan Enhanced K-Nearest Neighbour (EKNN)
5. Evaluasi: Confusion Matrix, Classification Report, ROC multi-kelas
6. Visualisasi: Before dan After preprocessing & segmentasi per kelas.
"""

# pyright: reportMissingImports=false
# type: ignore
try:
    from google.colab import files
except ImportError:
    files = None

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import joblib
# Install IPython jika belum ada
try:
    from IPython.display import display
except ImportError:
    # Fallback jika tidak ada IPython
    display = print

import warnings
warnings.filterwarnings('ignore')

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, ConfusionMatrixDisplay
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import train_test_split
from itertools import cycle
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

plt.rcParams['figure.figsize'] = (6, 4)
plt.rcParams['figure.dpi'] = 120

print("Libraries loaded.")

"""# EDA"""

# Pertama, mari kita periksa struktur internal archive.zip
import zipfile
import os

# Path dataset - sesuaikan dengan lokasi file Anda
dataset_zip_path = "archive.zip"  # jika dataset.zip ada di folder yang sama
# dataset_zip_path = r"d:\SEMESTER 5\MACHINE LEARNING\dataset.zip"  # atau gunakan path lengkap

# Unzip dataset
if os.path.exists(dataset_zip_path):
    print(f"Extracting {dataset_zip_path}...")
    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:
        zip_ref.extractall('.')
    print("Extraction complete!")
    base_dir = "./dataset"
else:
    print(f"Error: {dataset_zip_path} tidak ditemukan!")
    print(f"Pastikan file dataset.zip ada di folder: {os.getcwd()}")

# Periksa apa yang sebenarnya ada di direktori /content/ setelah unzip
print("Listing contents of /content/ after unzip:")
for root, dirs, files_list in os.walk('/content/'):
    level = root.replace('/content/', '').count(os.sep)
    indent = ' ' * 2 * level
    print(f'{indent}{os.path.basename(root)}/')
    subindent = ' ' * 2 * (level + 1)
    for file in files_list[:5]:  # batasi output
        print(f'{subindent}{file}')
print("\n")

base_dir = "archive/data jagung/data jagung train"   # BUKAN /content/dataset
classes = os.listdir(base_dir)
class_counts = {}

for c in classes:
    class_path = os.path.join(base_dir, c)
    num_files = len(os.listdir(class_path))
    class_counts[c] = num_files

class_counts

# Set direktori dataset utama. Kini daxtaset berada di /content/dataset
BASE_DIR = "archive/data jagung/data jagung train"

CLASS_MAP = {
    "daun sehat": "HL",   # Healthy Leaf
    "daun rusak": "DMG",  # Damaged Leaf
    "hawar daun": "NLB",  # Northern Leaf Blight
    "karat daun": "RS"    # Rust
}

print("Base directory for dataset processing:", BASE_DIR)
print("Kelas yang diharapkan:", CLASS_MAP)

# Kontruksi Data Awal
import pandas as pd

data = []
for class_name in classes:
    folder_path = os.path.join(BASE_DIR, class_name)
    if os.path.isdir(folder_path):
        for filename in os.listdir(folder_path):
            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                filepath = os.path.join(folder_path, filename)
                data.append({"filepath": filepath, "label": class_name})

df_raw = pd.DataFrame(data)

print(f"\nTotal gambar yang ditemukan: {len(df_raw)}")
print(f"Distribusi per kelas:\n{df_raw['label'].value_counts()}")

# Kontruksi Data Awal
import math
import os
import pandas as pd

# Reconstruct df_raw for EDA purposes here, as it's missing
data = []
for class_name in classes:
    folder_path = os.path.join(BASE_DIR, class_name)
    if os.path.isdir(folder_path):
        for filename in os.listdir(folder_path):
            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                filepath = os.path.join(folder_path, filename)
                data.append({"filepath": filepath, "label": class_name})

df_raw = pd.DataFrame(data)

num_samples = 4  # jumlah contoh tiap kelas
plt.figure(figsize=(14, 8))

total_slots = len(classes) * num_samples

rows = len(classes)
cols = num_samples

plt.suptitle("Contoh Gambar Tiap Kelas", fontsize=18, y=1.02)

plot_index = 1
for label in classes:
    class_files = df_raw[df_raw['label'] == label].sample(num_samples, random_state=42)

    for filepath in class_files["filepath"]:
        img = cv2.imread(filepath)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        plt.subplot(rows, cols, plot_index)
        plt.imshow(img)
        plt.title(label, fontsize=10)
        plt.axis("off")
        plot_index += 1

plt.tight_layout()
plt.show()

# Distribusi Kelas
classes = os.listdir(base_dir)
class_counts = {}

for c in classes:
    class_path = os.path.join(base_dir, c)
    num_files = len(os.listdir(class_path))
    class_counts[c] = num_files

print("\nJumlah gambar per kelas:")
print(class_counts)

class_counts = df_raw["label"].value_counts()

plt.figure(figsize=(8,5))
sns.countplot(data=df_raw, x="label", palette="viridis")
plt.title("Distribusi Jumlah Gambar per Kelas")
plt.xticks(rotation=20)
plt.ylabel("Jumlah Gambar")
plt.show()

print("\nJumlah gambar per kelas:")
print(class_counts)

# Visualisasi jumlah gambar
plt.figure(figsize=(8,5))
plt.bar(class_counts.index, class_counts.values, color='skyblue')
plt.title("Distribusi Jumlah Gambar per Kelas")
plt.xticks(rotation=45)
plt.ylabel("Jumlah Gambar")
plt.show()

# Tampilan contoh gambar per kelas
plt.figure(figsize=(12, 8))

for idx, c in enumerate(classes):
    folder_path = os.path.join(base_dir, c)
    img_name = os.listdir(folder_path)[0]  # Ambil 1 gambar pertama
    img_path = os.path.join(folder_path, img_name)

    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    plt.subplot(2, 2, idx+1)
    plt.imshow(img)
    plt.title(c)
    plt.axis("off")

plt.suptitle("Contoh Gambar Tiap Kelas", fontsize=16)
plt.show()

# Analisis Ukuran gambar (tinggi dan lebar)
sizes = []

for c in classes:
    folder_path = os.path.join(base_dir, c)
    for f in os.listdir(folder_path):
        img_path = os.path.join(folder_path, f)
        img = cv2.imread(img_path)

        if img is None:
            continue

        h, w = img.shape[:2]
        sizes.append((h, w))

sizes[:5]

# Visualisasi Distribusi ukuran gambar
heights = [s[0] for s in sizes]
widths  = [s[1] for s in sizes]

plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.hist(heights, bins=20, color='orange')
plt.title("Distribusi Tinggi Gambar")
plt.xlabel("Height (px)")
plt.ylabel("Frekuensi")

plt.subplot(1, 2, 2)
plt.hist(widths, bins=20, color='green')
plt.title("Distribusi Lebar Gambar")
plt.xlabel("Width (px)")
plt.ylabel("Frekuensi")

plt.show()

# Analisis Brightness (Tinhkat kecerahan gambar)
brightness = []

for c in classes:
    folder_path = os.path.join(base_dir, c)
    for f in os.listdir(folder_path):
        img_path = os.path.join(folder_path, f)
        img = cv2.imread(img_path)

        if img is None:
            continue

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        brightness.append(gray.mean())

plt.figure(figsize=(8,5))
plt.hist(brightness, bins=40, color='purple')
plt.title("Distribusi Brightness Gambar")
plt.xlabel("Brightness")
plt.ylabel("Jumlah Gambar")
plt.show()

# KONVERSI MENJADI FITUR NUMERIK SEDERHANA (UNTUK EDA ML)

def preprocess_to_vector(path, size=(64,64)):
    img = cv2.imread(path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img = cv2.resize(img, size)
    return img.flatten()

samples_per_class = 3
processed_rows = []

for label in classes:
    subset = df_raw[df_raw["label"] == label].sample(samples_per_class, random_state=42)

    for _, row in subset.iterrows():
        fp = row["filepath"]
        feat = preprocess_to_vector(fp)

        processed_rows.append([label] + feat.tolist())

# Buat dataframe
df_processed_preview = pd.DataFrame(processed_rows)
df_processed_preview.rename(columns={0: "label"}, inplace=True)

print("=== Contoh Fitur Setelah Preprocessing (Per Kelas) ===")
display(df_processed_preview.iloc[:, :25])  # menampilkan 25 fitur pertama agar tidak besar

# karena citra maka tidak akan ada missing value
df_features = df_processed_preview.drop('label', axis=1)
missing = df_features.isnull().sum().sum()
print("\nMissing values total:", missing)

if missing == 0:
    print("Tidak ada missing value (normal untuk dataset citra).")

# Outlier
plt.figure(figsize=(12,6))
sns.boxplot(data=df_features.iloc[:, :20], orient='h')
plt.title("Boxplot 20 Fitur Pertama untuk Outlier Check")
plt.show()

# Heatmap
plt.figure(figsize=(12,8))
corr = df_features.iloc[:, :30].corr()
sns.heatmap(corr, cmap="coolwarm")
plt.title("Heatmap Korelasi Fitur (30 Fitur Pertama)")
plt.show()

print("\n=== RINGKASAN EDA DATASET ===")
print("Jumlah total gambar :", len(df_raw))
print("Jumlah kelas        :", df_raw['label'].nunique())
print("Distribusi kelas    :\n", df_raw['label'].value_counts())
print("Rata-rata tinggi    :", np.mean(heights))
print("Rata-rata lebar     :", np.mean(widths))
print("Rata-rata brightness:", np.mean(brightness))
print("Dimensi fitur per gambar setelah preprocessing :", df_features.shape[1])

"""## Preprocessing"""

# ===================================================================================
# PRE-PROCESSING & SEGMENTASI
# ===================================================================================

def preprocess_image(img, target_size=(256, 256)):
    """
    Preprocessing citra daun jagung.

    Tahapan:
    1. Resize ke ukuran target (default 256x256, sejalan dengan PlantVillage).
    2. BGR (OpenCV) -> RGB.
    3. Normalisasi piksel ke [0, 1].
    4. Konversi ke grayscale.
    """
    img_resized = cv2.resize(img, target_size)
    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
    img_rgb_norm = img_rgb.astype("float32") / 255.0
    gray = cv2.cvtColor((img_rgb_norm * 255).astype("uint8"), cv2.COLOR_RGB2GRAY)
    return img_rgb_norm, gray


def segment_otsu(gray):
    """
    Segmentasi Otsu berbasis intensitas.

    1. Gaussian blur untuk mereduksi noise.
    2. Thresholding Otsu -> citra biner.
    """
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    _, otsu_binary = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    return otsu_binary

# ===================================================================================
# VISUALISASI: BEFORE–AFTER PREPROCESSING & OTSU SEGMENTATION
# ===================================================================================

import random

def show_before_after(path):
    """
    Menampilkan:
    - Citra asli (RGB)
    - Citra grayscale
    - Hasil segmentasi Otsu
    """
    img = cv2.imread(path)
    if img is None:
        print("Gagal membaca citra:", path)
        return

    rgb_norm, gray = preprocess_image(img)
    otsu_bin = segment_otsu(gray)

    plt.figure(figsize=(9, 3))

    plt.subplot(1, 3, 1)
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title("Original")
    plt.axis("off")

    plt.subplot(1, 3, 2)
    plt.imshow(gray, cmap="gray")
    plt.title("Grayscale")
    plt.axis("off")

    plt.subplot(1, 3, 3)
    plt.imshow(otsu_bin, cmap="gray")
    plt.title("Otsu Segmentation")
    plt.axis("off")

    plt.suptitle(os.path.basename(path))
    plt.tight_layout()
    plt.show()


print("Contoh visualisasi before-after per kelas:\n")
for folder_name in CLASS_MAP.keys():
    folder_path = os.path.join(BASE_DIR, folder_name)
    if not os.path.isdir(folder_path):
        continue
    files = [f for f in os.listdir(folder_path) if f.lower().endswith((".jpg", ".jpeg", ".png"))]
    if not files:
        continue
    sample_path = os.path.join(folder_path, random.choice(files))
    print(f"Kelas: {folder_name}, contoh file: {sample_path}")
    show_before_after(sample_path)

# ===================================================================================
# EKSTRAKSI FITUR: FINE, COARSE, DOR
# ===================================================================================

# ---------- FINE FEATURES (LBP-like rotation invariant) ----------
def extract_fine_features(gray, radius=1, neighbors=8, step=2):
    h, w = gray.shape
    codes = []

    for y in range(radius, h - radius, step):
        for x in range(radius, w - radius, step):
            center = gray[y, x]
            binary = []
            for n in range(neighbors):
                theta = 2 * np.pi * n / neighbors
                yy = int(round(y + radius * np.sin(theta)))
                xx = int(round(x + radius * np.cos(theta)))
                binary.append(1 if gray[yy, xx] >= center else 0)

            # Rotation invariant
            rotations = [
                int("".join(map(str, binary[i:] + binary[:i])), 2)
                for i in range(neighbors)
            ]
            codes.append(min(rotations))

    hist, _ = np.histogram(codes, bins=256, range=(0,256))
    hist = hist.astype("float32")
    hist /= (hist.sum() + 1e-8)
    return hist


# ---------- COARSE FEATURES (gradient magnitude) ----------
def extract_coarse_features(gray, num_bins=32):
    sobelx = cv2.Sobel(gray.astype("float32"), cv2.CV_32F, 1, 0, ksize=3)
    sobely = cv2.Sobel(gray.astype("float32"), cv2.CV_32F, 0, 1, ksize=3)
    magnitude = np.sqrt(sobelx**2 + sobely**2)

    hist, _ = np.histogram(magnitude, bins=num_bins, range=(0, magnitude.max()+1e-8))
    hist = hist.astype("float32")
    hist /= (hist.sum() + 1e-8)
    return hist


# ---------- DOR FEATURES ----------
def extract_dor_features(gray, window_size=5):
    assert window_size % 2 == 1, "window_size harus ganjil"

    pad = window_size // 2
    padded = np.pad(gray.astype("float32"), pad, mode="reflect")
    h, w = gray.shape
    dom_idx = []

    for y in range(h):
        for x in range(w):
            region = padded[y:y+window_size, x:x+window_size]
            center = padded[y+pad, x+pad]

            diffs = np.abs(region - center).flatten()
            dom_idx.append(np.argmax(diffs))

    num_pos = window_size * window_size
    hist, _ = np.histogram(dom_idx, bins=num_pos, range=(0, num_pos))
    hist = hist.astype("float32")
    hist /= (hist.sum() + 1e-8)
    return hist

# Wrapper
# ===================================================================================
# GABUNGKAN SEMUA FITUR MENJADI 1 VEKTOR
# ===================================================================================

def extract_all_features(gray):
    fine   = extract_fine_features(gray)
    coarse = extract_coarse_features(gray)
    dor    = extract_dor_features(gray)

    # Total fitur = 256 + 32 + 25 = 313 dimensi
    return np.concatenate([fine, coarse, dor]).astype("float32")

# Pipeline
# ===================================================================================
# PIPELINE UTAMA: PATH → PREPROCESSING → OTSU → FITUR
# ===================================================================================

def process_image_to_vector(path):
    img = cv2.imread(path)
    if img is None:
        print("Gagal membaca:", path)
        return None

    rgb_norm, gray = preprocess_image(img)
    otsu_mask = segment_otsu(gray)
    feat_vec = extract_all_features(gray)

    return feat_vec

# ===================================================================================
# MEMBANGUN X (FITUR) DAN y (LABEL)
# ===================================================================================

from tqdm import tqdm

X = []
y = []

print("Memulai ekstraksi fitur seluruh dataset...\n")

for fp, label in tqdm(zip(df_raw["filepath"], df_raw["label"]), total=len(df_raw)):
    feat = process_image_to_vector(fp)
    X.append(feat)
    y.append(label)

X = np.array(X)
y = np.array(y)

print("X shape:", X.shape)
print("Contoh fitur satu gambar:", X[0][:10])

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_encoded = le.fit_transform(y)

print("Label mapping:", dict(zip(le.classes_, le.transform(le.classes_))))

"""### Data Spliting & Scaling"""

# ===================================================================================
# TRAIN / VALIDATION / TEST SPLIT (STRATIFIED)
# ===================================================================================

from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y_encoded,
    test_size=0.30,
    random_state=42,
    stratify=y_encoded
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.50,
    random_state=42,
    stratify=y_temp
)

print("Train size :", X_train.shape)
print("Val size   :", X_val.shape)
print("Test size  :", X_test.shape)

# ===================================================================================
# SCALING (FIT HANYA DI TRAINING)
# ===================================================================================

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled   = scaler.transform(X_val)
X_test_scaled  = scaler.transform(X_test)

print("Scaling done! Train mean:", X_train_scaled.mean())
joblib.dump(scaler, "scaler.pkl")
print("Scaler saved to scaler.pkl")

# ===================================================================================
# 5-FOLD CROSS VALIDATION (MODEL: LOGISTIC REGRESSION)
# ===================================================================================

from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

logreg = LogisticRegression(max_iter=3000)

cv_scores = cross_val_score(logreg, X_train_scaled, y_train, cv=cv, scoring='accuracy')

print("CV scores:", cv_scores)
print("Mean Accuracy:", cv_scores.mean())
print("Std Dev:", cv_scores.std())

"""# Modeling"""

# ===================================================================================
# FUNGSI EVALUASI MODEL
# ===================================================================================

def evaluate_model(model, X_test, y_test, model_name="Model"):

    y_pred = model.predict(X_test)

    print("\n==============================")
    print(f"    EVALUATION: {model_name}")
    print("==============================")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=le.classes_))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
    plt.title(f"Confusion Matrix - {model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

    return accuracy_score(y_test, y_pred)

# ===================================================================================
# LOGISTIC REGRESSION
# ===================================================================================

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(max_iter=3000)
logreg.fit(X_train_scaled, y_train)

acc_logreg = evaluate_model(logreg, X_test_scaled, y_test, "Logistic Regression")

# ===================================================================================
# DECISION TREE
# ===================================================================================

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

acc_dt = evaluate_model(dt, X_test, y_test, "Decision Tree")

# ===================================================================================
# RANDOM FOREST
# ===================================================================================

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    random_state=42
)
rf.fit(X_train, y_train)

acc_rf = evaluate_model(rf, X_test, y_test, "Random Forest")

import subprocess
import sys

try:
    import xgboost
    print("XGBoost already installed")
except ImportError:
    print("Installing XGBoost...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "xgboost"])

# ===================================================================================
# XGBOOST CLASSIFIER
# ===================================================================================

from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.9,
    colsample_bytree=0.9,
    random_state=42,
    tree_method="hist"  # fast training
)

xgb.fit(X_train, y_train)

acc_xgb = evaluate_model(xgb, X_test, y_test, "XGBoost")

# ===================================================================================
# PERBANDINGAN AKURASI MODEL
# ===================================================================================

model_scores = {
    "Logistic Regression": acc_logreg,
    "Decision Tree": acc_dt,
    "Random Forest": acc_rf,
    "XGBoost": acc_xgb
}

import pandas as pd
df_scores = pd.DataFrame.from_dict(model_scores, orient="index", columns=["Accuracy"])

print("\n=== PERBANDINGAN AKURASI MODEL ===")
display(df_scores.sort_values(by="Accuracy", ascending=False))

"""# Hyperparameter Tunning"""

# ===================================================================================
# GRID SEARCH: LOGISTIC REGRESSION
# ===================================================================================

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

logreg_params = {
    "C": [0.01, 0.1, 1, 10],
    "solver": ["liblinear", "lbfgs"]
}

grid_logreg = GridSearchCV(
    LogisticRegression(max_iter=3000),
    logreg_params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid_logreg.fit(X_train_scaled, y_train)

print("Best Params (LogReg):", grid_logreg.best_params_)
print("Best Score:", grid_logreg.best_score_)

# ===================================================================================
# GRID SEARCH: DECISION TREE
# ===================================================================================

from sklearn.tree import DecisionTreeClassifier

dt_params = {
    "max_depth": [5, 10, 20, None],
    "criterion": ["gini", "entropy"],
    "min_samples_split": [2, 5, 10]
}

grid_dt = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    dt_params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid_dt.fit(X_train, y_train)

print("Best Params (Decision Tree):", grid_dt.best_params_)
print("Best Score:", grid_dt.best_score_)

# ===================================================================================
# RANDOM SEARCH: RANDOM FOREST
# ===================================================================================

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

rf_params = {
    "n_estimators": [100, 200, 300, 400],
    "max_depth": [10, 20, 30, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "bootstrap": [True, False]
}

rand_rf = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    rf_params,
    n_iter=20,
    cv=5,
    scoring="accuracy",
    n_jobs=-1,
    random_state=42
)

rand_rf.fit(X_train, y_train)

print("Best Params (Random Forest):", rand_rf.best_params_)
print("Best Score:", rand_rf.best_score_)

# ===================================================================================
# RANDOM SEARCH: XGBOOST
# ===================================================================================

from xgboost import XGBClassifier

xgb_params = {
    "n_estimators": [200, 300, 400],
    "learning_rate": [0.01, 0.05, 0.1],
    "max_depth": [4, 6, 8],
    "subsample": [0.7, 0.8, 0.9],
    "colsample_bytree": [0.7, 0.8, 0.9]
}

rand_xgb = RandomizedSearchCV(
    XGBClassifier(tree_method="hist", random_state=42),
    xgb_params,
    n_iter=20,
    cv=5,
    scoring="accuracy",
    n_jobs=-1,
    random_state=42
)

rand_xgb.fit(X_train, y_train)

print("Best Params (XGBoost):", rand_xgb.best_params_)
print("Best Score:", rand_xgb.best_score_)

# ===================================================================================
# MODEL FINAL EVALUATION
# ===================================================================================

best_logreg = grid_logreg.best_estimator_
best_dt     = grid_dt.best_estimator_
best_rf     = rand_rf.best_estimator_
best_xgb    = rand_xgb.best_estimator_

acc_best_logreg = evaluate_model(best_logreg, X_test_scaled, y_test, "Best Logistic Regression")
acc_best_dt     = evaluate_model(best_dt, X_test, y_test, "Best Decision Tree")
acc_best_rf     = evaluate_model(best_rf, X_test, y_test, "Best Random Forest")
acc_best_xgb    = evaluate_model(best_xgb, X_test, y_test, "Best XGBoost")

tuned_results = {
    "LogReg (Tuned)": acc_best_logreg,
    "DecisionTree (Tuned)": acc_best_dt,
    "RandomForest (Tuned)": acc_best_rf,
    "XGBoost (Tuned)": acc_best_xgb
}

df_tuned = pd.DataFrame.from_dict(tuned_results, orient="index", columns=["Accuracy"])
print("\n=== PERBANDINGAN AKURASI MODEL SETELAH TUNING ===")
display(df_tuned.sort_values("Accuracy", ascending=False))

"""# SHAP EXPLAINABILITY"""

try:
    import shap
    print("SHAP already installed")
except ImportError:
    print("Installing SHAP...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "shap"])

# ===================================================================================
# SHAP EXPLAINABILITY UNTUK XGBOOST
# ===================================================================================

import shap

explainer_xgb = shap.TreeExplainer(best_xgb)
shap_values_xgb = explainer_xgb.shap_values(X_test)

# Plot summary SHAP
shap.summary_plot(shap_values_xgb, X_test, plot_type="bar")

shap.summary_plot(shap_values_xgb, X_test)

idx = 5  # misal sample ke-5
shap.force_plot(
    explainer_xgb.expected_value[0], # Specify class for expected_value
    shap_values_xgb[idx][:, 0],     # Specify class for shap_values
    X_test[idx],
    matplotlib=True
)

# Ranking fitur berdasarkan mean(|SHAP|)
shap_abs = np.abs(shap_values_xgb).mean(axis=0)
feature_order = np.argsort(shap_abs)[::-1]

print("Top 10 Most Important Features:", feature_order[:10])

print(feature_order[:10])

print(np.array(shap_values_xgb).shape)
# 54 = jumlah sampel test
# 313 = jumlah fitur (FINE + COARSE + DOR)
# 4 = jumlah kelas (daun sehat, daun rusak, hawar, karat)

best_feature = feature_order[0]  # 257

shap.dependence_plot(
    best_feature,
    shap_values_xgb[:, :, 0],  # SHAP values untuk kelas 0
    X_test
)

top_features = feature_order[:3]   # [257, 15, 47]

for f in top_features:
    shap.dependence_plot(
        f,
        shap_values_xgb[:, :, 0],  # fokus kelas 0
        X_test
    )

class_names = ["Daun Sehat", "Daun Rusak", "Hawar Daun", "Karat Daun"]

for cls in range(4):
    print("Dependence Plot untuk kelas:", class_names[cls])
    shap.dependence_plot(
        best_feature,
        shap_values_xgb[:, :, cls],
        X_test
    )

# ===================================================================================
# SHAP UNTUK RANDOM FOREST
# ===================================================================================

explainer_rf = shap.TreeExplainer(best_rf)
shap_values_rf = explainer_rf.shap_values(X_test)

shap.summary_plot(shap_values_rf[:, :, 1], X_test)  # kelas 1 contoh

"""# download"""

try:
    import joblib
    print("Joblib already installed")
except ImportError:
    print("Installing joblib...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "joblib"])

# Simpan model dan dependencies
joblib.dump(best_xgb, "xgb_best_model.pkl")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(le, "label_encoder.pkl")

print("Models saved successfully!")

# Download jika di Colab
if files is not None:
    files.download("xgb_best_model.pkl")
    files.download("scaler.pkl")
    files.download("label_encoder.pkl")
    print("Files downloaded!")
else:
    print("Files saved to current directory (not in Colab)")


